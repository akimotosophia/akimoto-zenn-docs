---
title: "AWS Lambdaã§ã©ã“ã¾ã§å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ãŒã§ãã‚‹ã‹è©¦ã—ã¦ã¿ãŸ"
emoji: "ğŸš€"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["AWS", "Lambda", "ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹", "ãƒ‡ãƒ¼ã‚¿å‡¦ç†"]
published: true
---

## ã¯ã˜ã‚ã«

AWS Lambdaã¯ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ã§ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã§ãã‚‹ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã§ã™ã€‚ãã®ãŸã‚ã€ã¡ã‚‡ã£ã¨ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ETLå‡¦ç†ã®å®Ÿè¡ŒåŸºç›¤ã¨ã—ã¦ã‚‚æ¡ç”¨ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚ã—ã‹ã—ã€Lambdaã«ã¯ã„ãã¤ã‹åˆ¶é™ãŒã‚ã‚Šã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚„ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãªã©ã®éæ©Ÿèƒ½è¦ä»¶ã‚’æŠŠæ¡ã›ãšLambdaã‚’æ¡ç”¨ã—ã¦ã—ã¾ã†ã¨è½ã¨ã—ç©´ã«ã¯ã¾ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

ã“ã®è¨˜äº‹ã§ã¯Lambdaã§ã©ã“ã¾ã§ã®ãƒ•ã‚¡ã‚¤ãƒ«ã€å‡¦ç†ãŒã§ãã‚‹ã®ã‹ã‚’æ¤œè¨¼ã—ã¦ã¿ã‚ˆã†ã¨æ€ã„ã¾ã™ã€‚

## AWS Lambdaã®åˆ¶é™ã«ã¤ã„ã¦

AWS Lambdaã«ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªåˆ¶é™ãŒã‚ã‚Šã¾ã™ï¼š

- **å®Ÿè¡Œæ™‚é–“**: æœ€å¤§15åˆ†
- **ãƒ¡ãƒ¢ãƒª**: 128MBã€œ10,240MBï¼ˆ10GBï¼‰
- **ä¸€æ™‚ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸(/tmp)**: æœ€å¤§10GB
- **ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚µã‚¤ã‚º**: åœ§ç¸®æ™‚50MBã€å±•é–‹æ™‚250MB

ã“ã‚Œã‚‰ã®åˆ¶é™ã®ä¸­ã§ã€ã©ã“ã¾ã§å¤§å®¹é‡ã®ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãŒå¯èƒ½ãªã®ã‹ã‚’æ¤œè¨¼ã—ã¾ã™ã€‚

## ã‚„ã£ã¦ã¿ãŸã“ã¨

### æ¤œè¨¼ç’°å¢ƒã®æº–å‚™

1. Lambdaé–¢æ•°ã®è¨­å®š
   - ãƒ¡ãƒ¢ãƒª:10GB
   - ä¸€æ™‚ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸:5GB
   - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ:15åˆ†
   - ãƒ©ãƒ³ã‚¿ã‚¤ãƒ :python3.12
   - ãƒ¬ã‚¤ãƒ¤ãƒ¼:DuckDB([PyPI](https://pypi.org/project/duckdb/#files) åœ§ç¸®æ™‚18.5MB â€»2025å¹´11æœˆ12æ—¥æ™‚ç‚¹)

2. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
   - ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º:24GB
   - ãƒ‡ãƒ¼ã‚¿å½¢å¼:csv
   - ã‚«ãƒ©ãƒ :no(int),name(str),created_by(date)

### ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‡¦ç†ã®å®Ÿè£…

DuckDBã®ä¸¦åˆ—å‡¦ç†ãƒ»ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚’æ´»ã‹ã—ã€S3ã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿S3ã«ç›´æ¥æ›¸ãè¾¼ã‚€å½¢ã§å®Ÿè£…ã—ã¦ã¿ã¾ã—ãŸã€‚

::: details å®Ÿè£…ä¾‹
```python
# ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‡¦ç†ã®ã‚³ãƒ¼ãƒ‰ä¾‹
import json
import os
import boto3
from boto3.s3.transfer import TransferConfig
import duckdb
import tempfile

# S3è»¢é€è¨­å®š
transfer_config = TransferConfig(
    multipart_threshold=50 * 1024 * 1024,
    max_concurrency=10,
    multipart_chunksize=10 * 1024 * 1024,
    num_download_attempts=5,
    max_io_queue=1000,
    io_chunksize=256 * 1024,
    use_threads=True
)

s3_client = boto3.client('s3')


def lambda_handler(event, context):
    """
    å¤§å®¹é‡CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‡¦ç†ã‚’è¡Œã†Lambdaé–¢æ•°ï¼ˆDuckDB S3ç›´æ¥èª­ã¿è¾¼ã¿ç‰ˆï¼‰
    """
    try:
        # S3ã‚¤ãƒ™ãƒ³ãƒˆã‹ã‚‰æƒ…å ±ã‚’å–å¾—
        if 'Records' in event:
            bucket = event['Records'][0]['s3']['bucket']['name']
            key = event['Records'][0]['s3']['object']['key']
        else:
            bucket = event.get('bucket', os.environ.get('SOURCE_BUCKET'))
            key = event.get('key')

        dest_bucket = os.environ.get('DEST_BUCKET')

        print(f"Processing file: s3://{bucket}/{key}")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’å–å¾—
        response = s3_client.head_object(Bucket=bucket, Key=key)
        file_size_mb = response['ContentLength'] / (1024 * 1024)
        print(f"File size: {file_size_mb:.2f} MB")

        # DuckDBã§å‡¦ç†ï¼ˆãƒ¡ãƒ¢ãƒªåˆ¶é™è¨­å®šï¼‰
        conn = duckdb.connect(':memory:')

        # AWSèªè¨¼æƒ…å ±ã‚’å–å¾—ï¼ˆDuckDBç”¨ï¼‰
        session = boto3.Session()
        credentials = session.get_credentials()

        # DuckDBè¨­å®š
        print("Configuring DuckDB...")

        # ãƒ›ãƒ¼ãƒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®šï¼ˆLambdaç’°å¢ƒç”¨ï¼‰
        os.makedirs('/tmp/duckdb_home', exist_ok=True)
        conn.execute("SET home_directory='/tmp/duckdb_home'")

        # ãƒ¡ãƒ¢ãƒªåˆ¶é™ï¼ˆLambda ãƒ¡ãƒ¢ãƒªã®70%ç¨‹åº¦ï¼‰
        lambda_memory_mb = int(os.environ.get('AWS_LAMBDA_FUNCTION_MEMORY_SIZE', '10240'))
        duckdb_memory_mb = int(lambda_memory_mb * 0.7)
        conn.execute(f"SET memory_limit='{duckdb_memory_mb}MB'")

        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
        os.makedirs('/tmp/duckdb_temp', exist_ok=True)
        conn.execute("SET temp_directory='/tmp/duckdb_temp'")

        # ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ï¼ˆLambda vCPUæ•°ï¼‰
        threads = 6
        conn.execute(f"SET threads={threads}")

        # é †åºä¿è¨¼ãªã—ã«ã—ã€ä¸¦åˆ—æ€§ã‚¢ãƒƒãƒ—
        conn.execute("SET preserve_insertion_order=false")

        # httpfsæ‹¡å¼µã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»ãƒ­ãƒ¼ãƒ‰ï¼ˆS3ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ç”¨ï¼‰
        print("Loading DuckDB httpfs extension...")
        conn.execute("INSTALL httpfs")
        conn.execute("LOAD httpfs")

        # AWSèªè¨¼æƒ…å ±ã‚’è¨­å®š
        conn.execute(f"SET s3_region='ap-northeast-1'")
        conn.execute(f"SET s3_access_key_id='{credentials.access_key}'")
        conn.execute(f"SET s3_secret_access_key='{credentials.secret_key}'")

        if credentials.token:
            conn.execute(f"SET s3_session_token='{credentials.token}'")

        # S3ãƒ‘ã‚¹æ§‹ç¯‰
        s3_path = f"s3://{bucket}/{key}"

        print(f"Processing CSV directly from S3: {s3_path}")

        # DuckDBã‹ã‚‰ç›´æ¥S3ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        valid_key = f"cleansed/{key}"
        error_key = f"error/{key}"

        valid_s3_path = f"s3://{dest_bucket}/{valid_key}"
        error_s3_path = f"s3://{dest_bucket}/{error_key}"

        # æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥S3ã«æ›¸ãè¾¼ã¿ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆãªã—ï¼‰
        print(f"Exporting valid data directly to S3: {valid_s3_path}")
        conn.execute(f"""
            COPY (
                SELECT no, name, created_date
                FROM read_csv_auto('{s3_path}',
                    header=true,
                    all_varchar=true,
                    parallel=true,
                    ignore_errors=false,
                    sample_size=10000
                )
                WHERE
                    -- noã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆnullã§ãªã„ã€intã§ã‚ã‚‹ã“ã¨ï¼‰
                    no IS NOT NULL
                    AND TRIM(no) != ''
                    AND TRY_CAST(no AS INTEGER) IS NOT NULL
                    -- nameã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆ20æ–‡å­—ä»¥å†…ï¼‰
                    AND LENGTH(name) <= 20
                    -- created_dateã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆYYYY/MM/DDå½¢å¼ã€æœ‰åŠ¹ãªæ—¥ä»˜ï¼‰
                    AND REGEXP_MATCHES(created_date, '^[0-9]{{4}}/[0-9]{{2}}/[0-9]{{2}}$')
                    AND TRY_CAST(REPLACE(created_date, '/', '-') AS DATE) IS NOT NULL
            ) TO '{valid_s3_path}' (HEADER, DELIMITER ',', FORMAT CSV)
        """)
        print(f"Valid data uploaded: {valid_s3_path}")

        # ã‚¨ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥S3ã«æ›¸ãè¾¼ã¿ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆãªã—ï¼‰
        print(f"Exporting error data directly to S3: {error_s3_path}")
        conn.execute(f"""
            COPY (
                SELECT no, name, created_date
                FROM read_csv_auto('{s3_path}',
                    header=true,
                    all_varchar=true,
                    parallel=true,
                    ignore_errors=false,
                    sample_size=10000
                )
                WHERE
                    NOT (
                        no IS NOT NULL
                        AND TRIM(no) != ''
                        AND TRY_CAST(no AS INTEGER) IS NOT NULL
                        AND LENGTH(name) <= 20
                        AND REGEXP_MATCHES(created_date, '^[0-9]{{4}}/[0-9]{{2}}/[0-9]{{2}}$')
                        AND TRY_CAST(REPLACE(created_date, '/', '-') AS DATE) IS NOT NULL
                    )
            ) TO '{error_s3_path}' (HEADER, DELIMITER ',', FORMAT CSV)
        """)
        print(f"Error data uploaded: {error_s3_path}")

        conn.close()

        # ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã¯å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ï¼ˆå¿…è¦ãªå ´åˆï¼‰
        # ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã¯å–å¾—ã—ãªã„ï¼ˆé«˜é€ŸåŒ–å„ªå…ˆï¼‰
        print("Counting records from output files...")
        #valid_count = conn.execute(f"SELECT COUNT(*) FROM read_csv_auto('{valid_s3_path}')")
        #error_count = conn.execute(f"SELECT COUNT(*) FROM read_csv_auto('{error_s3_path}')")
        #total_count = valid_count + error_count

        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        import shutil
        if os.path.exists('/tmp/duckdb_temp'):
            shutil.rmtree('/tmp/duckdb_temp')
        if os.path.exists('/tmp/duckdb_home'):
            shutil.rmtree('/tmp/duckdb_home')

        print(f"Completed. Total: {total_count}, Valid: {valid_count}, Error: {error_count}")

        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': 'Success',
                'source': f"s3://{bucket}/{key}",
                'valid_output': f"s3://{dest_bucket}/{valid_key}",
                'error_output': f"s3://{dest_bucket}/{error_key}",
                'file_size_mb': round(file_size_mb, 2),
                'total_lines': total_count,
                'valid_lines': valid_count,
                'error_lines': error_count,
                'processing_mode': 'duckdb_s3_streaming'
            })
        }

    except Exception as e:
        print(f"Error: {str(e)}")
        import traceback
        traceback.print_exc()
        raise
```
:::

## çµæœ

- å®Ÿè¡Œæ™‚é–“:12åˆ†33ç§’ï¼ˆã†ã¡ã€æ­£å¸¸ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›ãŒ6åˆ†24ç§’ï¼‰
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:726MB

çµæœã®ã‚ˆã†ã«ã€24GBã®ãƒ‡ãƒ¼ã‚¿ã‚‚1GBä»¥ä¸‹ã®ãƒ¡ãƒ¢ãƒªã€5GBä»¥ä¸‹ã®ä¸€æ¬¡é ˜åŸŸï¼ˆå®Ÿéš›ã«ã¯ã‚‚ã£ã¨å°‘ãªã„å¯èƒ½æ€§ã‚‚ã‚ã‚Šï¼‰ã§å‡¦ç†ã‚’å®Œäº†ã™ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸã€‚

### æ³¨æ„ç‚¹

- Lambdaå†…éƒ¨ã«å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã›ãšã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°çš„ã«å‡¦ç†ã—ã¦ã„ã‚‹ãŸã‚ã€ãƒ©ãƒ³ãƒ€ãƒ ã‚¢ã‚¯ã‚»ã‚¹ãŒç™ºç”Ÿã™ã‚‹ã‚ˆã†ãªå‡¦ç†ã«ã¯é©ç”¨ãŒé›£ã—ã„ã¨æ€ã„ã¾ã™ã€‚
  - ä»Šå›ã¯ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã®ã‚«ã‚¦ãƒ³ãƒˆã‚’çœç•¥ã—ãŸã®ã‚‚ã€å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å†åº¦èª­ã¿è¾¼ã‚€å¿…è¦ãŒã‚ã‚ŠLambdaã®å®Ÿè¡Œæ™‚é–“ã®åˆ¶ç´„ä¸Šã«å¼•ã£ã‹ã‹ã‚‹ãŸã‚ã§ã™ã€‚
- é‡è¤‡æ’é™¤ã‚„ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ãªã©ã‚’ã™ã‚‹å ´åˆã‚‚tmpé ˜åŸŸãŒååˆ†ã«ãªã„ã¨Diskå®¹é‡ä¸è¶³ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚EFSã®ä½¿ç”¨ã‚’æ¤œè¨ã—ã¾ã—ã‚‡ã†ã€‚
- Lambdaã®ãƒ¡ãƒ¢ãƒªæ•°ã‚’ä¸‹ã’ã¦ã—ã¾ã†ã¨ã€vCPUã‚‚ä¸‹ãŒã£ã¦ã—ã¾ã„ä¸¦åˆ—å‡¦ç†ãŒã§ããªããªã‚Šé…ããªã‚Šã¾ã™ã€‚æœ€å¤§ã®6vCPUã‚’ç¶­æŒã™ã‚‹ã«ã¯9GBãã‚‰ã„ã¯å‰²ã‚Šå½“ã¦ãªã„ã¨ã„ã‘ãªã„ã‚ˆã†ã§ã™ã€‚
- å®‰å®šã—ãŸå‡¦ç†ã‚’å„ªå…ˆã™ã‚‹ãªã‚‰ãã‚‚ãã‚‚Fargateã‚’æ¤œè¨ã—ã¾ã—ã‚‡ã†ã€‚

## å‚è€ƒè¨˜äº‹

- [Lambda ã‚¯ã‚©ãƒ¼ã‚¿](https://docs.aws.amazon.com/ja_jp/lambda/latest/dg/gettingstarted-limits.html)
